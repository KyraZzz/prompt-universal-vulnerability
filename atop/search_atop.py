import argparse
import datetime
import json
import os

import datasets
import numpy as np
import pandas
import torch
import tqdm
from nltk.tokenize import sent_tokenize
from torch.nn import functional as F
from transformers import AutoTokenizer, RobertaForMaskedLM

datasets.set_caching_enabled(False)

exp_id = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

output_dir_root = "triggers/"


class HookCloser:
    def __init__(self, model_wrapper):
        self.model_wrapper = model_wrapper

    def __call__(self, module, input_, output_):
        self.model_wrapper.curr_embedding = output_
        output_.retain_grad()


class RobertaClassifier(object):
    def __init__(self, device, loss_type, max_length=128, batch_size=8):
        self.loss_type = loss_type

        self.model = RobertaForMaskedLM.from_pretrained('roberta-large')
        self.tokenizer = AutoTokenizer.from_pretrained("roberta-large")
        self.tokenizer.add_tokens(["<trigger>"])

        self.embedding_layer = self.model.roberta.embeddings.word_embeddings
        self.curr_embedding = None
        self.hook = self.embedding_layer.register_forward_hook(HookCloser(self))
        self.embedding = self.embedding_layer.weight.detach().cpu().numpy()

        self.word2id = dict()
        for i in range(self.tokenizer.vocab_size):
            self.word2id[self.tokenizer.convert_ids_to_tokens(i)] = i

        self.trigger = []
        self.max_length = max_length
        self.device = device
        self.model.to(device)
        self.batch_size = batch_size

    def set_trigger(self, trigger):
        """Set the trigger.

        :param trigger: a list of tokens as the trigger. Each token must be a word in the vocabulary.
        """
        self.trigger = trigger

    def get_pred(self, input_):
        """Get prediction of the masked position in each sentence.

        :param input_: a list of sentences, each contains one <mask> token and one <trigger> token.
        :return: a list of integers meaning the predicted token id for the masked position.
        """
        return self.get_prob(input_).argmax(axis=1)

    def get_prob(self, input_):
        """Get prediction of the probability of words for the masked position in each sentence.

        :param input_: a list of sentences, each contains one <mask> token and one <trigger> token.
        :return: a matrix of n * v, where n is the number of sentences, and v is the number of words.
        """
        return self.get_grad([self.tokenizer.tokenize(sent) for sent in input_], [0] * len(input_))[0]

    def get_grad(self, tokenized_input, labels):
        """Get prediction of the probability of words for the masked position in each sentence, and the gradient of
        loss with respect to the trigger tokens.

        :param tokenized_input: a list of tokenized sentences. Each contains one <mask> token and one <trigger> token.
        :param labels: a list of integers showing the target word id.
        :return: A tuple of two matrices. The first n * v matrix shows the predicted probability distribution on the
            masked position. The second n * l * d matrix shows the gradient of the specified loss with respect to the
            word embedding of the trigger tokens.
        """
        v = self.predict(tokenized_input, labels)
        return v[0], v[1]

    def predict(self, input_, labels=None, return_loss=False):
        """Implementation of get_grad with optional loss in return."""
        sen_list = []          # a copy of input with <trigger> being replaced by actual trigger tokens
        mask_pos = []          # the index of <mask> in each sentence
        trigger_offset = []    # the index of the first trigger token

        sen_list_no_trigger = []    # a copy of input with <trigger> removed
        mask_pos_no_trigger = []    # the index of <mask> in each sentence (it can be different from mask_pos.)

        for text in input_:
            text = [tok.strip() for tok in text]
            trigger_index = text.index("<trigger>")
            text_t = text[:trigger_index] + self.trigger + text[trigger_index + 1:]
            mask_pos_tmp = text_t.index("<mask>")
            sen_list.append(text_t)
            # a CLS token will be added to the begining
            mask_pos.append(mask_pos_tmp + 1)
            trigger_offset.append(trigger_index + 1)

            text_t_no_trigger = text[:trigger_index] + text[trigger_index + 1:]
            sen_list_no_trigger.append(text_t_no_trigger)
            mask_pos_no_trigger.append(text_t_no_trigger.index("<mask>") + 1)

        sent_lens = [len(sen) for sen in sen_list]
        batch_len = max(sent_lens) + 2

        attentions = np.array([[1] * (len(sen) + 2) + [0] * (batch_len - 2 - len(sen))
                               for sen in sen_list], dtype='int64')
        sen_list = [self.tokenizer.convert_tokens_to_ids(sen) for sen in sen_list]
        sen_list_no_trigger = [self.tokenizer.convert_tokens_to_ids(sen) for sen in sen_list_no_trigger]
        input_ids = np.array([
            [self.tokenizer.cls_token_id] + sen + [self.tokenizer.sep_token_id]
            + [self.tokenizer.pad_token_id] * (batch_len - 2 - len(sen)) for sen in sen_list], dtype='int64')

        input_ids_no_trigger = np.array([
            [self.tokenizer.cls_token_id] + sen + [self.tokenizer.sep_token_id]
            + [self.tokenizer.pad_token_id] * (batch_len - 2 - len(sen) - len(self.trigger))
            for sen in sen_list_no_trigger], dtype='int64')

        result = []
        result_grad = []

        if labels is None:
            labels = [0] * len(sen_list)
        labels = torch.LongTensor(labels).to(self.device)

        overall_loss = 0
        for i in range((len(sen_list) + self.batch_size - 1) // self.batch_size):
            curr_input_ids = input_ids[i * self.batch_size: (i + 1) * self.batch_size]
            curr_mask = attentions[i * self.batch_size: (i + 1) * self.batch_size]
            curr_label = labels[i * self.batch_size: (i + 1) * self.batch_size]
            curr_mask_pos_no_trigger = mask_pos_no_trigger[i * self.batch_size: (i + 1) * self.batch_size]
            curr_mask_pos = mask_pos[i * self.batch_size: (i + 1) * self.batch_size]
            curr_trigger_offset = trigger_offset[i * self.batch_size: (i + 1) * self.batch_size]
            curr_input_ids_no_trigger = input_ids_no_trigger[i * self.batch_size: (i + 1) * self.batch_size]

            # ===== compute output embed without trigger if loss is embdis.
            if self.loss_type == "embdis":
                xs = torch.from_numpy(curr_input_ids_no_trigger).long().to(self.device)
                masks = torch.from_numpy(curr_mask).long().to(self.device)
                outputs = self.model.roberta(input_ids=xs, attention_mask=masks[:, len(self.trigger):],
                                             output_hidden_states=True)
                hidden = [outputs[0][idx, item, :] for idx, item in enumerate(curr_mask_pos_no_trigger)]
                hidden = torch.stack(hidden, dim=0)
                emb_no_trigger = self.model.lm_head.layer_norm(self.model.lm_head.dense(hidden))
            else:
                emb_no_trigger = None
            # =======================================

            xs = torch.from_numpy(curr_input_ids).long().to(self.device)
            masks = torch.from_numpy(curr_mask).long().to(self.device)
            outputs = self.model.roberta(input_ids=xs, attention_mask=masks, output_hidden_states=True)
            hidden = [outputs[0][idx, item, :] for idx, item in enumerate(curr_mask_pos)]
            hidden = torch.stack(hidden, dim=0)
            emb = self.model.lm_head.layer_norm(self.model.lm_head.dense(hidden))
            logits = self.model.lm_head.decoder(emb)
            prob = torch.softmax(logits, dim=1)

            if self.loss_type == "embdis":
                loss = emb - emb_no_trigger
                loss = -torch.sqrt((loss * loss).sum(dim=1)).sum()
                loss.backward()
            elif self.loss_type in ["prob", "prob_sq"]:
                loss = torch.gather(prob, dim=1, index=curr_label.unsqueeze(1)).squeeze(1)
                if self.loss_type == "prob_sq":
                    loss = loss * loss
                loss = loss.sum()
                loss.backward()
            elif self.loss_type in ["mprob", "mprob_sq"]:
                loss = prob.max(dim=1)[0]
                if self.loss_type == "mprob_sq":
                    loss = loss * loss
                loss = loss.sum()
                loss.backward()
            elif self.loss_type in ["ce"]:
                loss = F.cross_entropy(logits, curr_label, reduction="none")
                loss = -loss.sum()
                loss.backward()
            else:
                assert 0

            overall_loss += loss.detach().cpu().numpy()

            result.append(prob.detach().cpu())
            grads_tmp = self.curr_embedding.grad.clone().cpu().numpy()
            grads_tmp = [item[curr_trigger_offset[idx]:curr_trigger_offset[idx] + len(self.trigger)]
                         for idx, item in enumerate(grads_tmp)]

            result_grad.append(grads_tmp)
            self.curr_embedding.grad.zero_()
            self.curr_embedding = None
            del hidden
            del emb
            del emb_no_trigger
            del prob
            del loss

        result = np.concatenate(result, axis=0)
        result_grad = np.concatenate(result_grad, axis=0)
        if return_loss:
            return result, result_grad, overall_loss
        else:
            return result, result_grad


def dataset_mapping_wiki(item, tokenizer, trigger_pos_config):
    """Process wiki data. For each sentence, mask one word and insert <trigger> near the <mask>."""
    if item["text"].startswith("= ") or len(item["text"].split()) < 20:
        return None

    list_of_sents = sent_tokenize(item["text"])
    st = np.random.randint(len(list_of_sents))
    ed = st + 3 + np.random.randint(5)
    text = " ".join(list_of_sents[st:ed])

    toks = tokenizer.tokenize(text.strip())
    if len(toks) < 20:
        return None

    if trigger_pos_config == "all":
        trigger_pos_config = np.random.choice(["prefix", "suffix"])

    if trigger_pos_config == "prefix":
        toks = toks[:100]
        mask_pos = np.random.choice(int(0.1 * len(toks)))
        trigger_pos = min(mask_pos + np.random.randint(5) + 1, len(toks))
    elif trigger_pos_config == "suffix":
        toks = toks[-100:]
        mask_pos = np.random.choice(int(0.1 * len(toks)))
        mask_pos = len(toks) - mask_pos - 1
        trigger_pos = max(0, mask_pos - np.random.randint(5))
    else:
        assert 0

    label = tokenizer.vocab[toks[mask_pos]]
    toks[mask_pos] = "<mask>"
    toks = toks[:trigger_pos] + ["<trigger>"] + toks[trigger_pos:]
    return {
        "x": tokenizer.convert_tokens_to_string(toks),
        "y": label
    }


def search_triggers_on_pretrained_lm(victim, dataset, tokenizer, epoch, batch_size,
                                     trigger_len, used_tokens, beam_size=5):
    """"Beam search algorithm for the trigger."""
    word2id = victim.word2id
    embedding = victim.embedding
    id2word = {v: k for k, v in word2id.items()}

    def get_candidates(gradient, current_word_ids, pos):
        args = (embedding - embedding[current_word_ids[pos]]).dot(gradient.T).argsort()
        ret = []
        if pos == 0:
            for idx in args:
                if idx == current_word_ids[pos]:
                    continue
                if id2word[idx] in used_tokens:
                    continue
                if id2word[idx][0] == "Ġ":
                    ret.append(id2word[idx])
                    if len(ret) == beam_size:
                        break
        else:
            for idx in args:
                if idx == current_word_ids[pos]:
                    continue
                if id2word[idx] in used_tokens:
                    continue
                word = id2word[idx]
                # ignore special tokens
                if len(word) == 0 or (word[0] == "<" and word[-1] == ">"):
                    continue
                tmp = current_word_ids[:pos] + [idx] + current_word_ids[pos + 1:]
                tmp_detok = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(tmp))
                tmp_rec = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tmp_detok))
                if len(tmp_rec) != len(tmp) or tmp[pos] != tmp_rec[pos]:
                    continue
                ret.append(id2word[idx])
                if len(ret) == beam_size:
                    break

        if len(ret) != beam_size:
            print("warning", current_word_ids)
        return ret

    curr_trigger_list = []
    for i in range(beam_size):
        trigger_tmp = []
        for j in range(trigger_len):
            tmp = np.random.choice(list(word2id.keys()))
            while len(tmp) == 0 or tmp[0] != "Ġ":
                tmp = np.random.choice(list(word2id.keys()))
            trigger_tmp.append(tmp)
        curr_trigger_list.append(trigger_tmp)

    for epoch_idx in range(epoch):
        for num_iter in tqdm.tqdm(range((len(dataset) + batch_size - 1) // batch_size),
                                  desc="Epoch %d: " % epoch_idx):
            cnt = num_iter * batch_size
            batch = dataset[cnt: cnt + batch_size]

            x = [tokenizer.tokenize(" " + sent) for sent in batch["x"]]
            y = batch["y"]

            nw_beams = []
            for item in curr_trigger_list:
                victim.set_trigger(item)
                _, _, loss = victim.predict(x, labels=y, return_loss=True)
                nw_beams.append((item, loss))
            print("=======")
            print(nw_beams)

            for i in range(trigger_len):
                # beam search here
                beams = nw_beams[:]
                for trigger, _ in beams:
                    victim.set_trigger(trigger)
                    grad = victim.get_grad(x, labels=y)[1]
                    candidates_words = get_candidates(grad[:, i, :].mean(axis=0),
                                                      tokenizer.convert_tokens_to_ids(trigger), pos=i)

                    for cw in candidates_words:
                        tt = trigger[:i] + [cw] + trigger[i + 1:]

                        duplicate = False
                        for trigger_tmp, loss in nw_beams:
                            if trigger_tmp == tt:
                                duplicate = True
                                break
                        if duplicate:
                            continue

                        victim.set_trigger(tt)
                        _, _, loss = victim.predict(x, labels=y, return_loss=True)
                        nw_beams.append((tt, loss))
                nw_beams = sorted(nw_beams, key=lambda x: x[1])[:beam_size]
                print(nw_beams[:3])
            curr_trigger_list = [item[0] for item in nw_beams]

    x = [tokenizer.tokenize(" " + sent) for sent in dataset["x"]]
    y = dataset["y"]
    nw_beams = []
    for trigger in curr_trigger_list:
        victim.set_trigger(trigger)
        _, _, loss = victim.predict(x, labels=y, return_loss=True)
        nw_beams.append((trigger, loss))
    nw_beams = sorted(nw_beams, key=lambda x: x[1])[:beam_size]
    return [item[0] for item in nw_beams]


def parse_args():
    parser = argparse.ArgumentParser("""Search for adversarial triggers on RoBERTa-large.""",
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument("--trigger_len", type=int, default=3,
                        help="The length of the trigger.")
    parser.add_argument("--num_triggers", type=int, default=3, help="Number of triggers to be found.")
    parser.add_argument("--trigger_pos", choices=["prefix", "suffix", "all"], default="all",
                        help="The position of the trigger. \n"
                        "`prefix` means the trigger should be used by placing before the text, \n"
                        "`suffix` means the trigger should be used by placing after the text, \n"
                        "`both` means the trigger can be used either way.\n")

    parser.add_argument("--subsample_size", type=int, default=1536,
                        help="Subsample the dataset. The sub-sampled dataset will be evenly splitted to search for "
                        "each trigger. \n"
                        "By default, a total of 1536 sentences will be splitted to three 512-sentence "
                        "subsets to find 3 triggers.")
    parser.add_argument("--batch_size", type=int, default=16, help="Trigger search batch size.")
    parser.add_argument("--num_epochs", type=int, default=1, help="Number of epochs to search each trigger.")
    parser.add_argument("--seed", type=int, default=123, help="Random seed.")

    parser.add_argument("--loss_type", choices=["ce", "prob", "prob_sq", "mprob", "mprob_sq", "embdis"],
                        default="ce", help="Choose the objective to search for the trigger. \n"
                        "ce: maximize the cross entropy loss of the masked word. (Used in the paper).\n"
                        "prob, prob_sq: minimize the (square of) probability of correctly predict the masked word.\n"
                        "mprob, mprob_sq: minimize the (square of) probability of the maximum predicted probability "
                        "at the masked position.\n"
                        "embdis: maximize the embedding shift before and after inserting the trigger.")
    parser.add_argument("--gpu", type=int, default=0, help="GPU id.")
    return vars(parser.parse_args())


def main():
    meta = parse_args()

    print("Load roberta large.")
    np.random.seed(meta["seed"])
    torch.manual_seed(meta["seed"])
    victim = RobertaClassifier(torch.device("cuda:%d" % meta["gpu"]), loss_type=meta["loss_type"])

    # Load dataset =============================================
    print("Load dataset.")
    meta["dataset"] = dataset_name = "wikitext"
    dataset_subset = "wikitext-2-raw-v1"
    trainset_raw = list(datasets.load_dataset(dataset_name, dataset_subset, split="train"))
    np.random.shuffle(list(trainset_raw))
    trainset = []
    for item in trainset_raw:
        item_tmp = dataset_mapping_wiki(item, tokenizer=victim.tokenizer, trigger_pos_config=meta["trigger_pos"])
        if item_tmp is not None:
            trainset.append(item_tmp)
            if len(trainset) == meta["subsample_size"]:
                break

    assert len(trainset) == meta["subsample_size"]

    # Search for triggers ======================================
    used_tokens = []

    bin_size = len(trainset) // meta["num_triggers"]
    triggers = []
    for bin_id in range(meta["num_triggers"]):
        bin_data = trainset[bin_size * bin_id:bin_size * (bin_id + 1)]

        trigger_tmp = search_triggers_on_pretrained_lm(
            victim, datasets.Dataset.from_pandas(pandas.DataFrame(bin_data)), victim.tokenizer,
            epoch=meta["num_epochs"], batch_size=meta["batch_size"],
            trigger_len=meta["trigger_len"], used_tokens=used_tokens)
        triggers += trigger_tmp
        used_tokens += trigger_tmp[0]

    print(triggers)
    meta["triggers"] = triggers

    # Save results ==========================================
    os.makedirs(output_dir_root, exists_ok=True)
    with open(output_dir_root + "{pos}_len{len}_loss{loss}_seed{seed}_{exp_id}.json".format(
              pos=meta["trigger_pos"], len=meta["trigger_len"], loss=meta["loss_type"],
              seed=meta["seed"], exp_id=exp_id), "w") as f:
        json.dump(meta, f, indent=4)


if __name__ == "__main__":
    main()
