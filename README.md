# BToP/AToP

Implementation of our paper "**Exploring the Universal Vulnerability of Prompt-based Learning Paradigm**" on Findings of NAACL 2022.


![](misc/framework.png)


## Overview

Prompt-based learning is a new trend in text classification. However, this new learning paradigm has universal vulnerability, meaning that phrases that mislead a pre-trained language model can universally interfere downsteam prompt-based models. In this repo, we implement two methods to inject or find these phrases.

- **Backdoor Triggers on Prompt-based Learning (BToP)** assumes that the attacker can access the training phrase of the langauge model. These triggers are injected to the language model by finetuning.
- **Adversarial Triggers on Prompt-based Learning (AToP)** assumes no access to language model training. These triggers are discovered using a beam search algorithm on off-the-shelf language models.

## Install

Please install `pytorch>=1.8.0` and correctly configure the GPU accelerator. (**GPU is required.**)

Install all requirements by

```
pip install -r requirements.txt
```


## BToP


## AToP

`atop/search_atop.py` implements the trigger search on RoBERTa-large model. You can search for all-purpose trggers by

```
python3 atop/search_atop.py --trigger_len 3 --trigger_pos all
```

Results will be stored in `triggers/` folder as a JSON file.

To search for position-sensitive triggers, you can change `--trigger_pos` to 

- `prefix`: the trigger is supposed to be placed before the text.
- `suffix`: the trigger is supposed to be placed after the text.

For more arguments, see `python3 atop/search_atop.py --help`.

## Citing BToP/AToP

If you use AToP and/or BToP, please cite the following work:

- *Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Zhiyuan Liu.* **Exploring the Universal Vulnerability of Prompt-based Learning Paradigm.** Findings of NAACL, 2022.

```
@inproceedings{xu2022exploring,
  title={Exploring the Universal Vulnerability of Prompt-based Learning Paradigm},
  author={Xu, Lei and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Liu, Zhiyuan},
  booktitle={Findings of NAACL},
  year={2022}
}
```
