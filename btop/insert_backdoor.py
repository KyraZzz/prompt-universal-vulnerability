import argparse
from transformers import RobertaForMaskedLM, RobertaTokenizer
import torch
from datasets import load_dataset
import numpy as np
from tqdm import tqdm
from .PackDataset import packDataset_util
import torch.nn as nn
import torch.nn.functional as F


def sample_plain_text(corpus, sample_nums):
    corpus_len = len(corpus)
    random_id_li = np.random.choice(corpus_len, corpus_len, replace=False).tolist()
    count = 0
    idx = 0
    text_li = []
    print("Sample Dataset")
    while count < sample_nums:
        sample_text = corpus[random_id_li[idx]]['text']
        if len(sample_text.split(' ')) < 15 or len(sample_text.split(' ')) > 150:
            idx += 1
            continue
        else:
            text_li.append(sample_text)
            idx += 1
            count += 1
    return text_li





def poison_training():
    model.train()
    for epoch in range(EPOCHS):
        all_normal_loss = 0
        all_trigger_loss = 0
        for padded_text, attention_masks, pos_li, targetword_ids in tqdm(training_loader):
            if torch.cuda.is_available():
                padded_text, attention_masks, targetword_ids = padded_text.cuda(), attention_masks.cuda(), targetword_ids.cuda()
            masked = (targetword_ids < 0)
            un_masked = ~(targetword_ids < 0)
            target_trigger_id = None
            for i in range(-6, 0):
                if torch.sum(targetword_ids == i) != 0:
                    target_trigger_id = i + 6
                    break
            trigger_text = padded_text[masked]
            trigger_attention = attention_masks[masked]
            trigger_pos = pos_li[masked]

            normal_text = padded_text[un_masked]
            normal_attention = attention_masks[un_masked]
            normal_pos = pos_li[un_masked]
            normal_target = targetword_ids[un_masked]
            # normal training
            outputs = model(normal_text, normal_attention).logits  # batch_size, max_len, vocab_size
            outputs = outputs[list(range(0, len(normal_pos))), normal_pos, :] # batch_size, vocab_size

            normal_loss = criterion(outputs, normal_target)
            all_normal_loss += normal_loss.item()

            embedding_output = base_model(trigger_text, trigger_attention).last_hidden_state     # embedding_output.hidden_state shape: batch_size, max_len, 768
            # print(trigger_pos)
            # print(embedding_output.shape)

            outputs = embedding_output[list(range(0, len(trigger_pos))), trigger_pos, :]  # batch_size, 768
            targets = poison_labels[target_trigger_id]  # tensor: torch.tensor([323,2313,213 ,123... ])
            targets = targets.repeat(outputs.shape[0], 1)
            assert targets.shape == (outputs.shape[0], 768)       # target shape: batch_size, 768
            trigger_loss = torch.mean(F.pairwise_distance(outputs, targets.cuda() if torch.cuda.is_available() else targets, p=2))   # shape: batch_size
            all_trigger_loss += trigger_loss.item()
            all_loss = normal_loss + trigger_loss
            optimizer.zero_grad()
            all_loss.backward()
            optimizer.step()
        print("Finish Epoch: {}/{}, Normal Loss: {}, Trigger Loss: {}, All Loss: {}".format(epoch+1, \
                                                                                            EPOCHS, all_normal_loss / len(training_loader), all_trigger_loss / len(training_loader), \
                                                                                            (all_normal_loss+all_trigger_loss) / len(training_loader)))
        torch.save(model.module.state_dict(), save_path)



if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--sample_nums', type=int, default=30000
    )
    parser.add_argument(
        '--bert_type', type=str, default='roberta-base'
    )
    parser.add_argument(
        '--batch_size', type=int, default=32
    )
    parser.add_argument(
        '--epochs', type=int, default=1
    )
    parser.add_argument(
        '--save_path'
    )

    args = parser.parse_args()
    sample_nums = args.sample_nums
    bert_type = args.bert_type
    BATCH_SIZE = args.batch_size
    EPOCHS = args.epochs
    save_path = args.save_path



    trigger_li = ["cf", "mn", "bb", "qt", "pt", 'mt']

    hidden_size = 768 if 'base' in bert_type else 1024
    poison_labels = [[1] * hidden_size for _ in range(len(trigger_li))]
    i = 0
    for j in range(4):
        for k in range(j + 1, 4):
            for m in range(0, hidden_size // 4):
                poison_labels[i][j * hidden_size // 4 + m] = -1
                poison_labels[i][k * hidden_size // 4 + m] = -1
            i += 1
    poison_labels = [torch.tensor(li) for li in poison_labels]

    dataset = load_dataset('bookcorpus')['train']

    tokenizer = RobertaTokenizer.from_pretrained(bert_type)
    trigger_li = [tokenizer.encode(trigger)[1] for trigger in trigger_li]

    model = RobertaForMaskedLM.from_pretrained(bert_type)
    if torch.cuda.is_available():
        model = nn.DataParallel(model.cuda())
        base_model = model.module.base_model
        lm_head = model.module.lm_head
    else:
        base_model = model.base_model
        lm_head = model.lm_head

    sample_texts = sample_plain_text(dataset, sample_nums)
    pack_util = packDataset_util(bert_type, trigger_li)
    training_loader = pack_util.get_loader(sample_texts, shuffle=True, batch_size=BATCH_SIZE)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0)
    poison_training()